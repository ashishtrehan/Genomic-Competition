{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import binarize\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location='/Users/ashishtrehan/Downloads/rstudio-export/'\n",
    "df=pd.read_csv(location+str('FLU_Challenge_vietnam.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = kmer_count(df['seqs'][0],100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length(row):\n",
    "    x = len(row['seqs'])\n",
    "    return x\n",
    "\n",
    "df['length']=df.apply (lambda row: length (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequence(row,n):\n",
    "    s=row['seqs']\n",
    "    k=len(s)-2*n\n",
    "    klist = []\n",
    "    for i in range(k):\n",
    "        kmer=s[i:i+n]\n",
    "        if not(kmer in klist) and (kmer in s[i+n:]):\n",
    "            klist.append(kmer)\n",
    "    return klist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['sequence']=df.apply (lambda row: sequence(row,3),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-91-cf325960ccbc>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-91-cf325960ccbc>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    #print z\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "for index,x in df.iterrows():\n",
    "    for z in x['sequence']:\n",
    "        #print z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq=[]\n",
    "for index,x in df.iterrows():\n",
    "    for z in x['sequence']:\n",
    "        seq.append(str(z))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def olpcount(string,pattern,casesensitive=True):\n",
    "    if casesensitive != True:\n",
    "        string  = string.lower()\n",
    "        pattern = string.lower()\n",
    "    l = len(pattern)\n",
    "    ct = 0\n",
    "    for c in range(0,len(string)):\n",
    "        if string[c:c+l] == pattern:\n",
    "            ct += 1\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index,x in df.iterrows():\n",
    "    for z in x['sequence']:\n",
    "        c = olpcount(df['seqs'][0],z)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sequence'][0])\n",
    "    #print len(x),x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for index, letter in enumerate(df['seqs'][0]):\n",
    "    #print(\"%i %s\" % (index, letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gencode = {\n",
    "    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M', 'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K', 'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n",
    "    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L', 'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q', 'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V', 'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E', 'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S', 'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "    'TAC':'Y', 'TAT':'Y', 'TAA':'_', 'TAG':'_', 'TGC':'C', 'TGT':'C', 'TGA':'_', 'TGG':'W'}\n",
    " \n",
    "# a function to translate a single codon\n",
    "def translate_codon(codon):\n",
    "    return gencode.get(codon.upper(), 'x')\n",
    " \n",
    "# a function to split a sequence into codons\n",
    "def split_into_codons(row, frame):\n",
    "    dna=row['seqs']\n",
    "    codons = []\n",
    "    for i in range(frame - 1, len(dna)-2, 3):\n",
    "        codon = dna[i:i+3]\n",
    "        codons.append(codon)\n",
    "    return ' '.join(codons)\n",
    " \n",
    "# a function to translate a dna sequence in a single frame\n",
    "def translate_dna_single(dna, frame=1):\n",
    "    codons = split_into_codons(dna, frame)\n",
    "    amino_acids = ''\n",
    "    for codon in codons:\n",
    "        amino_acids = amino_acids + translate_codon(codon)\n",
    "    return amino_acids\n",
    " \n",
    "# a function to translate a dna sequence in 3 forward frames\n",
    "def translate_dna(dna):\n",
    "    all_translations = []\n",
    "    for frame in range(1,4):\n",
    "        all_translations.append(translate_dna_single(dna, frame))\n",
    "    return all_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['codons']=df.apply (lambda row: split_into_codons(row,1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>seqs</th>\n",
       "      <th>length</th>\n",
       "      <th>codons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>agcaaaagcaggggtccaatctgtcaaaatggaaaaaatagtgctt...</td>\n",
       "      <td>1779</td>\n",
       "      <td>agc aaa agc agg ggt cca atc tgt caa aat gga aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>atggagaaaatagtgcttcttcttgcaatagttagccttgttaaaa...</td>\n",
       "      <td>1704</td>\n",
       "      <td>atg gag aaa ata gtg ctt ctt ctt gca ata gtt ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>atggagaaaatagtgcttcttcttgcaatggtcagccttgttaaaa...</td>\n",
       "      <td>1704</td>\n",
       "      <td>atg gag aaa ata gtg ctt ctt ctt gca atg gtc ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>atggagaaaatagtgcttcttcttgcaatagtcagccttgttaaaa...</td>\n",
       "      <td>1704</td>\n",
       "      <td>atg gag aaa ata gtg ctt ctt ctt gca ata gtc ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>atggagaaaatagtgcttcttcttgcaatagtcagccttgttaaaa...</td>\n",
       "      <td>1704</td>\n",
       "      <td>atg gag aaa ata gtg ctt ctt ctt gca ata gtc ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               seqs  length  \\\n",
       "0   0  agcaaaagcaggggtccaatctgtcaaaatggaaaaaatagtgctt...    1779   \n",
       "1   0  atggagaaaatagtgcttcttcttgcaatagttagccttgttaaaa...    1704   \n",
       "2   0  atggagaaaatagtgcttcttcttgcaatggtcagccttgttaaaa...    1704   \n",
       "3   0  atggagaaaatagtgcttcttcttgcaatagtcagccttgttaaaa...    1704   \n",
       "4   0  atggagaaaatagtgcttcttcttgcaatagtcagccttgttaaaa...    1704   \n",
       "\n",
       "                                              codons  \n",
       "0  agc aaa agc agg ggt cca atc tgt caa aat gga aa...  \n",
       "1  atg gag aaa ata gtg ctt ctt ctt gca ata gtt ag...  \n",
       "2  atg gag aaa ata gtg ctt ctt ctt gca atg gtc ag...  \n",
       "3  atg gag aaa ata gtg ctt ctt ctt gca ata gtc ag...  \n",
       "4  atg gag aaa ata gtg ctt ctt ctt gca ata gtc ag...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('features', FeaturePipeline(n_jobs=1,\n",
      "        transformer_list=[('onegrams', OneGrams(min_df=2)), ('mgrams-1', MGrams(min_df=2, ngrams=(2, 2))), ('mgrams-2', MGrams(min_df=2, ngrams=(3, 3)))],\n",
      "        transformer_weights=None)), ('forest', TreeFS(n_features=800)), ('svm', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=False,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
      "     verbose=0))])\n"
     ]
    }
   ],
   "source": [
    "class TreeFS(sklearn.base.BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_features=100):\n",
    "        self.forest = ExtraTreesClassifier(n_estimators=100)\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        self.forest.fit(df, y)\n",
    "        self.importances = self.forest.feature_importances_\n",
    "        self.indices = np.argsort(-self.importances)[0:self.n_features]\n",
    "        self.indices.sort()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):   \n",
    "        return X[:, self.indices]\n",
    "        \n",
    "class Ngrams(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, ngrams=(1,1),min_df=1, name=\"ngram\"):\n",
    "        self.min_df = min_df\n",
    "        self.ngrams = ngrams\n",
    "        self.name = name\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names_\n",
    "    \n",
    "    def _set_feature_names(self, features):\n",
    "        return [(self.name, x) for x in features]\n",
    "        \n",
    "        \n",
    "    def fit(self, df,y=None):\n",
    "        self.tfidf = CountVectorizer(ngram_range=self.ngrams, min_df=self.min_df, max_df=0.9,token_pattern=\"[a-zA-Z]{2,15}\")\n",
    "                                    #vocabulary=['New Girl'])\n",
    "                                     #stop_words=\"english\")\n",
    "                                     #token_pattern=\"[a-zA-Z]{2,15}\")\n",
    "        self.tfidf.fit(df)\n",
    "        self.feature_names_ = self._set_feature_names(self.tfidf.get_feature_names())\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df,y=None):\n",
    "        return binarize(self.tfidf.transform(df).todense())\n",
    "\n",
    "class OneGrams(Ngrams):\n",
    "    \n",
    "    def __init__(self, min_df=1):\n",
    "        super(OneGrams, self).__init__(min_df=min_df, name=\"onegram\")\n",
    "        \n",
    "\n",
    "class MGrams(Ngrams):\n",
    "    \n",
    "    def __init__(self, ngrams=(2,2), min_df=1):\n",
    "        super(MGrams, self).__init__(ngrams=ngrams,min_df=min_df,name=\"mgram\")\n",
    "        \n",
    "    def _set_feature_names(self, features):\n",
    "        return [(self.name, \"[, .:;/-]+\".join(x.split(\" \"))) for x in features]\n",
    "    \n",
    "\n",
    "class FeaturePipeline(FeatureUnion):\n",
    "    \n",
    "    def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):\n",
    "        super(FeaturePipeline, self).__init__(transformer_list, n_jobs, transformer_weights)\n",
    "        \n",
    "    def get_features(self):\n",
    "        feature_names = []\n",
    "        for name, trans in self.transformer_list:\n",
    "            feature_names.extend(trans.get_feature_names())\n",
    "        return feature_names\n",
    "\n",
    "forest = TreeFS(n_features=800)    \n",
    "svm = LinearSVC(fit_intercept=False, random_state=1)\n",
    "fp = FeaturePipeline(sklearn.pipeline._name_estimators([OneGrams(min_df=2), MGrams((2,2), min_df=2), MGrams((3,3),min_df=2)]))\n",
    "p = Pipeline([(\"features\",fp),(\"forest\",forest),(\"svm\",svm)])\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.fillna(3)\n",
    "#df_train[df['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "0    467\n",
       "1     48\n",
       "3    115\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['ID']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has 421 obversations and test set has 94\n"
     ]
    }
   ],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df_training=df[(df['ID']<3)]\n",
    "msk = np.random.rand(len(df_training)) < 0.8\n",
    "df_train=df_training[msk]\n",
    "df_test=df_training[~msk]\n",
    "print \"training set has {0} observations and test set has {1}\".format(len(df_train),len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features', FeaturePipeline(n_jobs=1,\n",
       "        transformer_list=[('onegrams', OneGrams(min_df=2)), ('mgrams-1', MGrams(min_df=2, ngrams=(2, 2))), ('mgrams-2', MGrams(min_df=2, ngrams=(3, 3)))],\n",
       "        transformer_weights=None)), ('forest', TreeFS(n_features=800)), ('svm', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=False,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=1, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.fit(df_train[\"codons\"], \n",
    "      df_train[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=list(reversed(sorted(zip(p.named_steps[\"svm\"].coef_.reshape((-1,)), \n",
    "           np.array([feature[1] for feature in p.named_steps[\"features\"].get_features()])[p.named_steps[\"forest\"].indices]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual,Predicted,First 10 of Sequence\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagggaa\n",
      "0.0 0.0 atggagagaa\n",
      "0.0 0.0 ccaatctgtc\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 gatcagattt\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 aatctgtcaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 aaaatggaga\n",
      "0.0 0.0 aaaatggata\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 ttcaatttgt\n",
      "0.0 0.0 aaatagtgct\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 1.0 tcaaaatgga\n",
      "0.0 0.0 ggttcaatct\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 agcgaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 ggttcaatct\n",
      "0.0 0.0 ggttcaatct\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 1.0 ggttcaatct\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagcc\n",
      "0.0 0.0 aaaatggaga\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggaaaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 ttcactctgt\n",
      "0.0 0.0 gcaggggttc\n",
      "0.0 0.0 ggtccaaact\n",
      "0.0 0.0 tctaatctgt\n",
      "0.0 1.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 caggggtata\n",
      "0.0 0.0 tagcaaaagc\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 agcaaaagca\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 aatcagcctt\n",
      "0.0 0.0 atggaaaaaa\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 aaatggagaa\n",
      "0.0 0.0 aatctgtcaa\n",
      "0.0 0.0 ggtctaatct\n",
      "0.0 0.0 ggtccaatct\n",
      "0.0 0.0 atggagaaaa\n",
      "0.0 0.0 ttcaatctgt\n",
      "0.0 0.0 ttcactctgt\n",
      "0.0 0.0 ggttcaaact\n",
      "1.0 1.0 atggagaaaa\n",
      "1.0 0.0 atggagagaa\n",
      "1.0 1.0 atggagaaaa\n",
      "1.0 1.0 tctgtcaaaa\n",
      "1.0 1.0 tctgtcaaaa\n",
      "1.0 1.0 tgtcaaaatg\n",
      "1.0 0.0 tctgtcaaaa\n",
      "1.0 1.0 aaatggagaa\n",
      "1.0 0.0 atggagaaaa\n",
      "1.0 0.0 atggagaaaa\n",
      "1.0 1.0 ttcactctgt\n",
      "1.0 1.0 ttcactctgt\n",
      "1.0 1.0 ttcactctgt\n"
     ]
    }
   ],
   "source": [
    "print \"Actual,Predicted,First 10 of Sequence\"\n",
    "for x,z,c in zip(df_test['ID'],p.predict(df_test['codons']),df_test['seqs']):\n",
    "    print x,z,c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92553191489361697"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test['ID'],p.predict(df_test['codons']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.text import Text  \n",
    "textList = Text(df[(df['ID']<3)]['codons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "textList.concordance('att')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['codons'].to_csv('/Users/ashishtrehan/Downloads/text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-33-01b0d755e78e>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-01b0d755e78e>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    size=num_features, min_count = min_word_count,             window = context, sample = downsampling)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "#model = word2vec.Word2Vec(textList, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-dd1f6d00d6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/gensim/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrpmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRpModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogentropy_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogEntropyModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mldamulticore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascontiguousarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m  \u001b[0;31m# utility fnc for pickling, common scipy operations etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitervalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name utils"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
